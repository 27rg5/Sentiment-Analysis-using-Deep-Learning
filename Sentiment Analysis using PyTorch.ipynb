{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53d7ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import contractions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "import gensim.downloader as api\n",
    "import gensim.models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2213aa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "056d61fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1+cu116'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2f701",
   "metadata": {},
   "source": [
    "##### VERSIONS OF OTHER LIBRARIES USED\n",
    "\n",
    "scikit-image : 0.19.2 <br>\n",
    "numpy : 1.21.5 <br>\n",
    "pandas : 1.4.2 <br>\n",
    "contractions : 0.1.73 <br>\n",
    "gensim : 4.1.2 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c125134d",
   "metadata": {},
   "source": [
    "# Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0fc2140",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.tsv', header=0, sep='\\t', quotechar='\"', on_bad_lines='skip', dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "802cecc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>1797882</td>\n",
       "      <td>R3I2DHQBR577SS</td>\n",
       "      <td>B001ANOOOE</td>\n",
       "      <td>2102612</td>\n",
       "      <td>The Naked Bee Vitmin C Moisturizing Sunscreen ...</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Love this, excellent sun block!!</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>18381298</td>\n",
       "      <td>R1QNE9NQFJC2Y4</td>\n",
       "      <td>B0016J22EQ</td>\n",
       "      <td>106393691</td>\n",
       "      <td>Alba Botanica Sunless Tanning Lotion, 4 Ounce</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Thank you Alba Bontanica!</td>\n",
       "      <td>The great thing about this cream is that it do...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>19242472</td>\n",
       "      <td>R3LIDG2Q4LJBAO</td>\n",
       "      <td>B00HU6UQAG</td>\n",
       "      <td>375449471</td>\n",
       "      <td>Elysee Infusion Skin Therapy Elixir, 2oz.</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great Product, I'm 65 years old and this is al...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>19551372</td>\n",
       "      <td>R3KSZHPAEVPEAL</td>\n",
       "      <td>B002HWS7RM</td>\n",
       "      <td>255651889</td>\n",
       "      <td>Diane D722 Color, Perm And Conditioner Process...</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>GOOD DEAL!</td>\n",
       "      <td>I use them as shower caps &amp; conditioning caps....</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>14802407</td>\n",
       "      <td>RAI2OIG50KZ43</td>\n",
       "      <td>B00SM99KWU</td>\n",
       "      <td>116158747</td>\n",
       "      <td>Biore UV Aqua Rich Watery Essence SPF50+/PA+++...</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>this soaks in quick and provides a nice base f...</td>\n",
       "      <td>This is my go-to daily sunblock. It leaves no ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094302</th>\n",
       "      <td>US</td>\n",
       "      <td>50113639</td>\n",
       "      <td>RZ7RZ02MTP4SL</td>\n",
       "      <td>B000050B70</td>\n",
       "      <td>185454094</td>\n",
       "      <td>Conair NE150NSCS Cordless Nose and Ear Hair Tr...</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Great Little Grooming Tool</td>\n",
       "      <td>After watching my Dad struggle with his scisso...</td>\n",
       "      <td>2000-11-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094303</th>\n",
       "      <td>US</td>\n",
       "      <td>52940456</td>\n",
       "      <td>R2IRC0IZ8YCE5T</td>\n",
       "      <td>B000050FF2</td>\n",
       "      <td>678848064</td>\n",
       "      <td>Homedics Envirascape Sound Spa Alarm Clock Radio</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Not bad for the price</td>\n",
       "      <td>Like most sound machines, the sounds choices a...</td>\n",
       "      <td>2000-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094304</th>\n",
       "      <td>US</td>\n",
       "      <td>47587881</td>\n",
       "      <td>R1U4ZSXOD228CZ</td>\n",
       "      <td>B000050B6U</td>\n",
       "      <td>862195513</td>\n",
       "      <td>Conair Instant Heat Curling Iron</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>89</td>\n",
       "      <td>97</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Best Curling Iron Ever</td>\n",
       "      <td>I bought this product because it indicated 30 ...</td>\n",
       "      <td>2000-11-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094305</th>\n",
       "      <td>US</td>\n",
       "      <td>53047750</td>\n",
       "      <td>R3SFJLZE09URWM</td>\n",
       "      <td>B000050FDE</td>\n",
       "      <td>195242894</td>\n",
       "      <td>Oral-B Professional Care 1000 Power Toothbrush</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>The best electric toothbrush ever, REALLY!</td>\n",
       "      <td>We have used Oral-B products for 15 years; thi...</td>\n",
       "      <td>2000-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094306</th>\n",
       "      <td>US</td>\n",
       "      <td>51193940</td>\n",
       "      <td>R1MEWK4I7YS5XK</td>\n",
       "      <td>B000050AUD</td>\n",
       "      <td>190668305</td>\n",
       "      <td>Sonicare PL-4 (4700) Sonic Toothbrush</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Smooth and shiny teeth!</td>\n",
       "      <td>I love this toothbrush. It's easy to use, and ...</td>\n",
       "      <td>2000-10-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5094307 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        marketplace customer_id       review_id  product_id product_parent  \\\n",
       "0                US     1797882  R3I2DHQBR577SS  B001ANOOOE        2102612   \n",
       "1                US    18381298  R1QNE9NQFJC2Y4  B0016J22EQ      106393691   \n",
       "2                US    19242472  R3LIDG2Q4LJBAO  B00HU6UQAG      375449471   \n",
       "3                US    19551372  R3KSZHPAEVPEAL  B002HWS7RM      255651889   \n",
       "4                US    14802407   RAI2OIG50KZ43  B00SM99KWU      116158747   \n",
       "...             ...         ...             ...         ...            ...   \n",
       "5094302          US    50113639   RZ7RZ02MTP4SL  B000050B70      185454094   \n",
       "5094303          US    52940456  R2IRC0IZ8YCE5T  B000050FF2      678848064   \n",
       "5094304          US    47587881  R1U4ZSXOD228CZ  B000050B6U      862195513   \n",
       "5094305          US    53047750  R3SFJLZE09URWM  B000050FDE      195242894   \n",
       "5094306          US    51193940  R1MEWK4I7YS5XK  B000050AUD      190668305   \n",
       "\n",
       "                                             product_title product_category  \\\n",
       "0        The Naked Bee Vitmin C Moisturizing Sunscreen ...           Beauty   \n",
       "1            Alba Botanica Sunless Tanning Lotion, 4 Ounce           Beauty   \n",
       "2                Elysee Infusion Skin Therapy Elixir, 2oz.           Beauty   \n",
       "3        Diane D722 Color, Perm And Conditioner Process...           Beauty   \n",
       "4        Biore UV Aqua Rich Watery Essence SPF50+/PA+++...           Beauty   \n",
       "...                                                    ...              ...   \n",
       "5094302  Conair NE150NSCS Cordless Nose and Ear Hair Tr...           Beauty   \n",
       "5094303   Homedics Envirascape Sound Spa Alarm Clock Radio           Beauty   \n",
       "5094304                   Conair Instant Heat Curling Iron           Beauty   \n",
       "5094305     Oral-B Professional Care 1000 Power Toothbrush           Beauty   \n",
       "5094306              Sonicare PL-4 (4700) Sonic Toothbrush           Beauty   \n",
       "\n",
       "        star_rating helpful_votes total_votes vine verified_purchase  \\\n",
       "0                 5             0           0    N                 Y   \n",
       "1                 5             0           0    N                 Y   \n",
       "2                 5             0           0    N                 Y   \n",
       "3                 5             0           0    N                 Y   \n",
       "4                 5             0           0    N                 Y   \n",
       "...             ...           ...         ...  ...               ...   \n",
       "5094302           5            10          10    N                 N   \n",
       "5094303           3            23          23    N                 N   \n",
       "5094304           5            89          97    N                 N   \n",
       "5094305           5            10          10    N                 N   \n",
       "5094306           5            23          23    N                 N   \n",
       "\n",
       "                                           review_headline  \\\n",
       "0                                               Five Stars   \n",
       "1                                Thank you Alba Bontanica!   \n",
       "2                                               Five Stars   \n",
       "3                                               GOOD DEAL!   \n",
       "4        this soaks in quick and provides a nice base f...   \n",
       "...                                                    ...   \n",
       "5094302                         Great Little Grooming Tool   \n",
       "5094303                              Not bad for the price   \n",
       "5094304                             Best Curling Iron Ever   \n",
       "5094305         The best electric toothbrush ever, REALLY!   \n",
       "5094306                            Smooth and shiny teeth!   \n",
       "\n",
       "                                               review_body review_date  \n",
       "0                         Love this, excellent sun block!!  2015-08-31  \n",
       "1        The great thing about this cream is that it do...  2015-08-31  \n",
       "2        Great Product, I'm 65 years old and this is al...  2015-08-31  \n",
       "3        I use them as shower caps & conditioning caps....  2015-08-31  \n",
       "4        This is my go-to daily sunblock. It leaves no ...  2015-08-31  \n",
       "...                                                    ...         ...  \n",
       "5094302  After watching my Dad struggle with his scisso...  2000-11-12  \n",
       "5094303  Like most sound machines, the sounds choices a...  2000-11-07  \n",
       "5094304  I bought this product because it indicated 30 ...  2000-11-02  \n",
       "5094305  We have used Oral-B products for 15 years; thi...  2000-11-01  \n",
       "5094306  I love this toothbrush. It's easy to use, and ...  2000-10-31  \n",
       "\n",
       "[5094307 rows x 15 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d0c8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Love this, excellent sun block!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The great thing about this cream is that it do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Great Product, I'm 65 years old and this is al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>I use them as shower caps &amp; conditioning caps....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This is my go-to daily sunblock. It leaves no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094302</th>\n",
       "      <td>5</td>\n",
       "      <td>After watching my Dad struggle with his scisso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094303</th>\n",
       "      <td>3</td>\n",
       "      <td>Like most sound machines, the sounds choices a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094304</th>\n",
       "      <td>5</td>\n",
       "      <td>I bought this product because it indicated 30 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094305</th>\n",
       "      <td>5</td>\n",
       "      <td>We have used Oral-B products for 15 years; thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094306</th>\n",
       "      <td>5</td>\n",
       "      <td>I love this toothbrush. It's easy to use, and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5094307 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body\n",
       "0                 5                   Love this, excellent sun block!!\n",
       "1                 5  The great thing about this cream is that it do...\n",
       "2                 5  Great Product, I'm 65 years old and this is al...\n",
       "3                 5  I use them as shower caps & conditioning caps....\n",
       "4                 5  This is my go-to daily sunblock. It leaves no ...\n",
       "...             ...                                                ...\n",
       "5094302           5  After watching my Dad struggle with his scisso...\n",
       "5094303           3  Like most sound machines, the sounds choices a...\n",
       "5094304           5  I bought this product because it indicated 30 ...\n",
       "5094305           5  We have used Oral-B products for 15 years; thi...\n",
       "5094306           5  I love this toothbrush. It's easy to use, and ...\n",
       "\n",
       "[5094307 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df.iloc[:, 7], df.iloc[:, 13]], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5df9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['star_rating','review_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b903e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIVIDED ALL THE REVIEWS INTO 3 CLASSES, AS WE DID IN HW1\n",
    "pd.options.mode.chained_assignment = None \n",
    "def labelClass(rating):\n",
    "    if rating == \"1\" or rating == \"2\":\n",
    "          return 1\n",
    "    if rating == \"3\" :\n",
    "          return 2\n",
    "    if rating == \"4\" or rating  == \"5\":\n",
    "          return 3\n",
    "df['class'] = df['star_rating'].map(labelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "505ecd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A BALANCED DATASET OF 60K REVIEWS, LIKE HW1\n",
    "class1 = df.loc[df['class'] == 1].sample(n=20000, random_state=1)\n",
    "class2 = df.loc[df['class'] == 2].sample(n=20000, random_state=1)\n",
    "class3 = df.loc[df['class'] == 3].sample(n=20000, random_state=1)\n",
    "df = pd.concat([class1, class2, class3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95aa32ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_body'] = df['review_body'].str.lower()  # CONVERT ALL REVEIWS TO LOWERCASE\n",
    "df['review_body'] = df['review_body'].astype(str)  \n",
    "df['review_body'] = df['review_body'].apply(lambda x: re.sub(re.compile('http\\S+|https\\S+'), \"\", x))  # REMOVE ALL URLs \n",
    "df['review_body'] = df['review_body'].apply(lambda x: re.sub(re.compile('<.*?>'), \"\", x))   # REMOVE ALL HTML TAGS\n",
    "df['review_body'] = df['review_body'].apply(lambda x: re.sub(re.compile(\"[^A-Za-z]\"),\" \", x))  # REMOVE ALL NON-ALPHABETICAL CHARACTERS\n",
    "df['review_body'] = df['review_body'].apply(lambda x: re.sub(re.compile(' +'),' ', x))  # REMOVES EXTRA SPACES IN REVIEWS\n",
    "df['review_body'] = df['review_body'].apply(lambda x: contractions.fix(x))  # PERFORM CONTRACTIONS ON REVIEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545cd2b",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd61a02",
   "metadata": {},
   "source": [
    "#### PART A (PRETRAINED WORD2VEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1f6bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_vec = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7611f1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8353004"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK SIMILARITY BETWEEN THE FOLLOWING 2 WORDS\n",
    "word_2_vec.similarity(\"gorgeous\", \"beautiful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ea745fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66321707"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK SIMILARITY BETWEEN THE FOLLOWING 2 WORDS\n",
    "word_2_vec.similarity(\"happy\", \"pleased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f1b8f80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('lady', 0.5354641079902649)\n",
      "('person', 0.529635488986969)\n",
      "('Woman', 0.513024628162384)\n",
      "('men', 0.4956325590610504)\n",
      "('policewoman', 0.4909151792526245)\n"
     ]
    }
   ],
   "source": [
    "neighbors = word_2_vec.most_similar(positive=['man', 'woman'], negative=['boy'], topn=5)\n",
    "for n in neighbors:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c23a3",
   "metadata": {},
   "source": [
    "#### PART B (WORD2VEC TRAINED ON OUR DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fd98eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT THE REVIEWS INTO INDIVIDUAL WORDS\n",
    "sentences = []\n",
    "for i in range(len(df['review_body'])):\n",
    "    sentences.append(df['review_body'].values[i].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a36c4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=sentences, vector_size=300, min_count=9, window=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b2f154b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85005796"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK SIMILARITY BETWEEN THE FOLLOWING 2 WORDS\n",
    "model.wv.similarity(\"gorgeous\", \"beautiful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6c5436f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90446115"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK SIMILARITY BETWEEN THE FOLLOWING 2 WORDS\n",
    "model.wv.similarity(\"happy\", \"pleased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7311b095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('lady', 0.5354641079902649)\n",
      "('person', 0.529635488986969)\n",
      "('Woman', 0.513024628162384)\n",
      "('men', 0.4956325590610504)\n",
      "('policewoman', 0.4909151792526245)\n"
     ]
    }
   ],
   "source": [
    "neighbors = word_2_vec.most_similar(positive=['man', 'woman'], negative=['boy'], topn=5)\n",
    "for n in neighbors:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79127de0",
   "metadata": {},
   "source": [
    "##### The trained Word2Vec model on our dataset seems to encode semantic similarities between words better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf585689",
   "metadata": {},
   "source": [
    "# Simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71d1ce",
   "metadata": {},
   "source": [
    "## Using TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d163c088",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Creating X data and y labels\n",
    "X = df['review_body']\n",
    "y = df['class']\n",
    "    \n",
    "# Applying TFIDF feature extraction on X\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7737d6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 34598) (12000, 34598) (48000,) (12000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=10)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c8888b",
   "metadata": {},
   "source": [
    "### Perceptron using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a80d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT A SINGLE PERCEPTRON MODEL ON OUR DATASET\n",
    "model1 = Perceptron(tol=1e-3, random_state=100)\n",
    "model1.fit(X_train, y_train)\n",
    "y_predict = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfda7f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.64      0.64      3981\n",
      "           2       0.58      0.53      0.55      4318\n",
      "           3       0.68      0.74      0.71      3701\n",
      "\n",
      "    accuracy                           0.63     12000\n",
      "   macro avg       0.63      0.64      0.63     12000\n",
      "weighted avg       0.63      0.63      0.63     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report1 = classification_report(y_predict, y_test, output_dict=True)\n",
    "print(classification_report(y_predict,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4d02558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\tPrecision\t\tRecall\t\t\tF1-score\n",
      "\n",
      "1     \t0.6394557823129252,\t0.6375282592313489,\t0.6384905660377359\n",
      "2     \t0.5762669342699448,\t0.531959240389069,\t0.5532273603082851\n",
      "3     \t0.681087762669963,\t0.7443934071872467,\t0.7113348825200103\n",
      "average\t0.6322701597509442,\t0.6379603022692216,\t0.6343509362886771 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Class\\tPrecision\\t\\tRecall\\t\\t\\tF1-score\\n')\n",
    "print('1     '+\"\\t\"+ str(report1['1']['precision'])+\",\\t\"+str(report1['1']['recall'])+\",\\t\"+ str(report1['1']['f1-score']))\n",
    "print('2     '+\"\\t\"+ str(report1['2']['precision'])+\",\\t\"+str(report1['2']['recall'])+\",\\t\"+ str(report1['2']['f1-score']))\n",
    "print('3     '+\"\\t\"+ str(report1['3']['precision'])+\",\\t\"+str(report1['3']['recall'])+\",\\t\"+ str(report1['3']['f1-score']))\n",
    "print('average' +\"\\t\"+str((report1['1']['precision']+report1['2']['precision']+report1['3']['precision'])/3)+\",\\t\"+str((report1['1']['recall']+report1['2']['recall']+report1['3']['recall'])/3)+\",\\t\"+str((report1['1']['f1-score']+report1['2']['f1-score']+report1['3']['f1-score'])/3), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418531ae",
   "metadata": {},
   "source": [
    "### SVM using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed5fb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT A SVM MODEL ON OUR DATASET\n",
    "model2 = LinearSVC(random_state=42)\n",
    "model2.fit(X_train, y_train)\n",
    "y_predict = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e93bdbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.70      0.71      4017\n",
      "           2       0.60      0.61      0.61      3894\n",
      "           3       0.78      0.77      0.78      4089\n",
      "\n",
      "    accuracy                           0.70     12000\n",
      "   macro avg       0.70      0.70      0.70     12000\n",
      "weighted avg       0.70      0.70      0.70     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report2 = classification_report(y_predict, y_test, output_dict=True)\n",
    "print(classification_report(y_predict,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3645d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\tPrecision\t\tRecall\t\t\tF1-score\n",
      "\n",
      "1     \t0.709498614260519,\t0.7010206621857107,\t0.7052341597796142\n",
      "2     \t0.5988459608630206,\t0.6129943502824858,\t0.6058375634517766\n",
      "3     \t0.7824474660074165,\t0.7740278796771827,\t0.7782149004179985\n",
      "average\t0.6969306803769854,\t0.6960142973817932,\t0.6964288745497965 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Class\\tPrecision\\t\\tRecall\\t\\t\\tF1-score\\n')\n",
    "print('1     '+\"\\t\"+ str(report2['1']['precision'])+\",\\t\"+str(report2['1']['recall'])+\",\\t\"+ str(report2['1']['f1-score']))\n",
    "print('2     '+\"\\t\"+ str(report2['2']['precision'])+\",\\t\"+str(report2['2']['recall'])+\",\\t\"+ str(report2['2']['f1-score']))\n",
    "print('3     '+\"\\t\"+ str(report2['3']['precision'])+\",\\t\"+str(report2['3']['recall'])+\",\\t\"+ str(report2['3']['f1-score']))\n",
    "print('average' +\"\\t\"+str((report2['1']['precision']+report2['2']['precision']+report2['3']['precision'])/3)+\",\\t\"+str((report2['1']['recall']+report2['2']['recall']+report2['3']['recall'])/3)+\",\\t\"+str((report2['1']['f1-score']+report2['2']['f1-score']+report2['3']['f1-score'])/3), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cecd195",
   "metadata": {},
   "source": [
    "## Word2Vec for feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e1f7b7",
   "metadata": {},
   "source": [
    "### Perceptron using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "081e5c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "sent = []\n",
    "\n",
    "for i in range(len(df['review_body'])):\n",
    "    split_sent = df['review_body'].values[i].split(' ')\n",
    "    for word in split_sent:\n",
    "        try:\n",
    "            sent.append(word_2_vec[word])\n",
    "        except:\n",
    "            sent.append(np.zeros(300))\n",
    "    sent = np.array(sent)\n",
    "    sent = np.mean(sent, axis=0)\n",
    "    data.append(sent)\n",
    "    sent = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9bb164b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 300) (12000, 300) (48000,) (12000,)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "data_label = df['class'].values\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(data, data_label, test_size = 0.2, random_state=10)\n",
    "print(X_train1.shape, X_test1.shape, y_train1.shape, y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0a2d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT A SINGLE PERCEPTRON MODEL ON OUR DATASET\n",
    "model3 = Perceptron(tol=1e-3, random_state=10)\n",
    "model3.fit(X_train1, y_train1)\n",
    "y_predict = model3.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf9e1831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.59      0.65      4776\n",
      "           2       0.61      0.54      0.57      4526\n",
      "           3       0.56      0.83      0.67      2698\n",
      "\n",
      "    accuracy                           0.63     12000\n",
      "   macro avg       0.63      0.66      0.63     12000\n",
      "weighted avg       0.64      0.63      0.62     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report3 = classification_report(y_predict, y_test1, output_dict=True)\n",
    "print(classification_report(y_predict,y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a84757d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\tPrecision\t\tRecall\t\t\tF1-score\n",
      "\n",
      "1     \t0.7150415721844293,\t0.5942211055276382,\t0.6490566037735849\n",
      "2     \t0.6131460110386352,\t0.5399911621741051,\t0.5742481203007519\n",
      "3     \t0.5557478368355995,\t0.8332097850259451,\t0.6667655346285036\n",
      "average\t0.627978473352888,\t0.6558073509092295,\t0.6300234195676134 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Class\\tPrecision\\t\\tRecall\\t\\t\\tF1-score\\n')\n",
    "print('1     '+\"\\t\"+ str(report3['1']['precision'])+\",\\t\"+str(report3['1']['recall'])+\",\\t\"+ str(report3['1']['f1-score']))\n",
    "print('2     '+\"\\t\"+ str(report3['2']['precision'])+\",\\t\"+str(report3['2']['recall'])+\",\\t\"+ str(report3['2']['f1-score']))\n",
    "print('3     '+\"\\t\"+ str(report3['3']['precision'])+\",\\t\"+str(report3['3']['recall'])+\",\\t\"+ str(report3['3']['f1-score']))\n",
    "print('average' +\"\\t\"+str((report3['1']['precision']+report3['2']['precision']+report3['3']['precision'])/3)+\",\\t\"+str((report3['1']['recall']+report3['2']['recall']+report3['3']['recall'])/3)+\",\\t\"+str((report3['1']['f1-score']+report3['2']['f1-score']+report3['3']['f1-score'])/3), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097a6fb",
   "metadata": {},
   "source": [
    "### SVM using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d85ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT A SVM MODEL ON OUR DATASET\n",
    "model4 = LinearSVC(random_state=100)\n",
    "model4.fit(X_train1, y_train1)\n",
    "y_predict = model4.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a9a3a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.70      0.66      0.68      4232\n",
      "           2       0.55      0.59      0.57      3732\n",
      "           3       0.72      0.72      0.72      4036\n",
      "\n",
      "    accuracy                           0.66     12000\n",
      "   macro avg       0.66      0.65      0.66     12000\n",
      "weighted avg       0.66      0.66      0.66     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report4 = classification_report(y_predict, y_test1, output_dict=True)\n",
    "print(classification_report(y_predict,y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14f4c166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\tPrecision\t\tRecall\t\t\tF1-score\n",
      "\n",
      "1     \t0.6984126984126984,\t0.6550094517958412,\t0.676015120107304\n",
      "2     \t0.5524335173105871,\t0.590032154340836,\t0.5706141487431978\n",
      "3     \t0.7181705809641533,\t0.7197720515361744,\t0.7189704244524192\n",
      "average\t0.6563389322291463,\t0.6549378858909506,\t0.6551998977676403 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Class\\tPrecision\\t\\tRecall\\t\\t\\tF1-score\\n')\n",
    "print('1     '+\"\\t\"+ str(report4['1']['precision'])+\",\\t\"+str(report4['1']['recall'])+\",\\t\"+ str(report4['1']['f1-score']))\n",
    "print('2     '+\"\\t\"+ str(report4['2']['precision'])+\",\\t\"+str(report4['2']['recall'])+\",\\t\"+ str(report4['2']['f1-score']))\n",
    "print('3     '+\"\\t\"+ str(report4['3']['precision'])+\",\\t\"+str(report4['3']['recall'])+\",\\t\"+ str(report4['3']['f1-score']))\n",
    "print('average' +\"\\t\"+str((report4['1']['precision']+report4['2']['precision']+report4['3']['precision'])/3)+\",\\t\"+str((report4['1']['recall']+report4['2']['recall']+report4['3']['recall'])/3)+\",\\t\"+str((report4['1']['f1-score']+report4['2']['f1-score']+report4['3']['f1-score'])/3), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60da4f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PERCEPTRON -\n",
      "Test accuracy when using TF-IDF: 63.24999999999999 %\n",
      "Test accuracy when using Word2Vec: 62.74999999999999 %\n"
     ]
    }
   ],
   "source": [
    "pred = model1.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred) * 100\n",
    "pred1 = model3.predict(X_test1)\n",
    "acc1 = accuracy_score(y_test1, pred1) * 100\n",
    "print(\"FOR PERCEPTRON -\")\n",
    "print(\"Test accuracy when using TF-IDF:\", acc, \"%\")\n",
    "print(\"Test accuracy when using Word2Vec:\", acc1, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b0a87fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR SVM -\n",
      "Test accuracy when using TF-IDF: 69.73333333333333 %\n",
      "Test accuracy when using Word2Vec: 65.65833333333333 %\n"
     ]
    }
   ],
   "source": [
    "pred3 = model2.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred3) * 100\n",
    "pred4 = model4.predict(X_test1)\n",
    "acc1 = accuracy_score(y_test1, pred4) * 100\n",
    "print(\"FOR SVM -\")\n",
    "print(\"Test accuracy when using TF-IDF:\", acc, \"%\")\n",
    "print(\"Test accuracy when using Word2Vec:\", acc1, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04574476",
   "metadata": {},
   "source": [
    "##### TF-IDF gives better results than Word2Vec for both Perceptron and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205c67c5",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f357f",
   "metadata": {},
   "source": [
    "#### PART A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33c3277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE ONE-HOT ENCODINGS OF THE 3 RATINGS\n",
    "enc_data_label = np.zeros((len(data_label), 3))\n",
    "for i in range(len(data_label)):\n",
    "    if data_label[i] == 1:\n",
    "        enc_data_label[i] = [0, 0, 1]\n",
    "    elif data_label[i] == 2:\n",
    "        enc_data_label[i] = [0, 1, 0]\n",
    "    elif data_label[i] == 3:\n",
    "        enc_data_label[i] = [1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cdec7e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 300) (12000, 300) (48000, 3) (12000, 3)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN-TEST-SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, enc_data_label, test_size = 0.2, random_state=10)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e81bd98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FNN IN PYTORCH\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(300, 100)   \n",
    "        self.dropout = nn.Dropout(0.2)   \n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)     # added dropout to reduce overfitting \n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "#         x = self.dropout(x)   # added dropout to reduce overfitting \n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3b486ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USING GPU\n",
    "print(torch.cuda.device_count())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "317f1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_model = FNN().to(device)\n",
    "fnn_model = fnn_model.to(device)   # using '.to(device)' to move the model from CPU to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6dde16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DATASET CLASS FOR DATALOADERS\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data1, data2):\n",
    "        self.data1 = data1\n",
    "        self.data2 = data2\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data1[idx]\n",
    "        y = self.data2[idx]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3fa78c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # BATCH SIZE FOR THIS MODEL\n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_dataset = Dataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c1435590",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optim=torch.optim.Adam(fnn_model.parameters(), lr = 0.001)  \n",
    "CEloss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55619c90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.4492083333333333\n",
      "Train accuracy: 0.47072916666666664\n",
      "Train accuracy: 0.4746458333333333\n",
      "Train accuracy: 0.47654166666666664\n",
      "Train accuracy: 0.47922916666666665\n",
      "Train accuracy: 0.479625\n",
      "Train accuracy: 0.48225\n",
      "Train accuracy: 0.48033333333333333\n",
      "Train accuracy: 0.4801458333333333\n",
      "Train accuracy: 0.48185416666666664\n",
      "Train accuracy: 0.48504166666666665\n",
      "Train accuracy: 0.48585416666666664\n",
      "Train accuracy: 0.48404166666666665\n",
      "Train accuracy: 0.48702083333333335\n",
      "Train accuracy: 0.4846875\n",
      "Train accuracy: 0.4861666666666667\n",
      "Train accuracy: 0.48685416666666664\n",
      "Train accuracy: 0.48741666666666666\n",
      "Train accuracy: 0.4875\n",
      "Train accuracy: 0.4895833333333333\n",
      "Train accuracy: 0.4875\n",
      "Train accuracy: 0.48814583333333333\n",
      "Train accuracy: 0.48820833333333336\n",
      "Train accuracy: 0.49\n",
      "Train accuracy: 0.4886041666666667\n",
      "Train accuracy: 0.4888958333333333\n",
      "Train accuracy: 0.562375\n",
      "Train accuracy: 0.6599791666666667\n",
      "Train accuracy: 0.6700416666666666\n",
      "Train accuracy: 0.6755416666666667\n",
      "Final test accuracy: 0.6591666666666667\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE FNN MODEL\n",
    "history_train = []\n",
    "history_test = []\n",
    "train_dataloader_len = len(train_dataloader)\n",
    "test_dataloader_len = len(test_dataloader)\n",
    "train_len = X_train.shape[0]\n",
    "test_len = X_test.shape[0]\n",
    "\n",
    "for epoch in range(30):  # loops over the complete dataset multiple times (which is the nummber of epochs)\n",
    "    fnn_model.train()     \n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    train_accuracy = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):  # loops over complete training dataset once \n",
    "        \n",
    "        inputs, label = data\n",
    "        inputs = inputs.float()    # convert double values to float\n",
    "        inputs, label = inputs.to(device), label.to(device)   \n",
    "\n",
    "        model_optim.zero_grad()\n",
    "        output = fnn_model(inputs)   # forward pass of model\n",
    "        output = output.to(device)\n",
    "        \n",
    "        loss1 = CEloss(output, label)     # loss calculation\n",
    "        loss1.backward()            # computes the gradient during the backward pass\n",
    "        model_optim.step()   # performs single optimization step\n",
    "\n",
    "        train_loss += loss1.item()   # adding accuracy values of all batches in an epoch\n",
    "        _, output = torch.max(output, 1)     # storing the index of maximum value in prediction to the variable 'output'\n",
    "        output = output.cpu().detach().numpy()     # loads the variable to cpu and converts it to a numpy array\n",
    "        label = label.cpu().detach().numpy()        \n",
    "        label = np.argmax(label, axis = 1)   # storing the index of maximum value in label to the variable 'label'\n",
    "        train_accuracy += accuracy_score(label, output)  # adding accuracy values of all batches in training dataset in an epoch\n",
    "    \n",
    "    train_loss = train_loss/train_dataloader_len\n",
    "    train_accuracy = train_accuracy/train_dataloader_len  # dividing accuracy by number of batches for training dataset\n",
    "    history_train.append((train_loss, train_accuracy))\n",
    "    \n",
    "    fnn_model.eval()     # model evaluation on test dataset\n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    correct_test = 0\n",
    "    with torch.no_grad():     # disables gradient calculation\n",
    "        for i, data in enumerate(test_dataloader, 0):    # loops over complete test dataset once\n",
    "            \n",
    "            inputs, label = data\n",
    "            inputs = inputs.float()\n",
    "            inputs, label = inputs.to(device), label.to(device)\n",
    "\n",
    "            pred = fnn_model(inputs)\n",
    "            pred = pred.to(device)\n",
    "            loss2 = CEloss(pred, label)\n",
    "\n",
    "            test_loss += loss2.item()\n",
    "            _, pred = torch.max(pred, 1)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            label = label.cpu().detach().numpy()\n",
    "            label = np.argmax(label, axis = 1)    \n",
    "            test_accuracy += accuracy_score(label, pred)\n",
    "            \n",
    "        test_loss = test_loss/test_dataloader_len\n",
    "        test_accuracy = test_accuracy/test_dataloader_len\n",
    "        history_test.append((test_loss, test_accuracy))\n",
    "    \n",
    "#     print(\"Epoch:\", (epoch+1))\n",
    "#     print(\"Train Loss:\", train_loss, \"\\tTrain Accuracy:\", train_accuracy)\n",
    "#     print(\"Test Loss:\", test_loss, \"\\tTest Accuracy:\", test_accuracy)\n",
    "#     print(\"=======================================================================================\")\n",
    "    print(\"Train accuracy:\", train_accuracy)\n",
    "\n",
    "print(\"Final test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325c43f4",
   "metadata": {},
   "source": [
    "#### PART B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c7bca5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the first 10 Word2Vec vectors for each review\n",
    "concat_data = []\n",
    "sent = []\n",
    "for i in range(len(df['review_body'])):\n",
    "    split_sent = df['review_body'].values[i].split(' ')\n",
    "    for word in split_sent[:10]:\n",
    "        try:\n",
    "            sent.append(word_2_vec[word])\n",
    "        except:\n",
    "            \n",
    "            sent.append(list(np.zeros(300)))   \n",
    "    if len(sent) < 10:\n",
    "        sent = np.concatenate([sent,np.zeros((10-len(sent), 300))])\n",
    "    sent = np.array(sent)\n",
    "    sent = sent.flatten()\n",
    "    concat_data.append(sent)\n",
    "    sent = []\n",
    "concat_data = np.array(concat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cbcc475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(3000, 100)\n",
    "        self.dropout = nn.Dropout(0.2)   \n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)     # added dropout to reduce overfitting \n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e352f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_model1 = FNN1().to(device)\n",
    "fnn_model1 = fnn_model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "af8b260c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 3000) (12000, 3000) (48000, 3) (12000, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(concat_data, enc_data_label, test_size = 0.2, random_state=10)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7e63e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_dataset = Dataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "20709191",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optim=torch.optim.Adam(fnn_model1.parameters(), lr = 0.001)\n",
    "CEloss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0a5c59a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.5382083333333333\n",
      "Train accuracy: 0.6067291666666667\n",
      "Train accuracy: 0.6413958333333334\n",
      "Train accuracy: 0.6778958333333334\n",
      "Train accuracy: 0.7139166666666666\n",
      "Train accuracy: 0.7483541666666667\n",
      "Train accuracy: 0.7807916666666667\n",
      "Train accuracy: 0.80425\n",
      "Train accuracy: 0.8216875\n",
      "Train accuracy: 0.8404583333333333\n",
      "Train accuracy: 0.8522708333333333\n",
      "Train accuracy: 0.8615\n",
      "Train accuracy: 0.874875\n",
      "Train accuracy: 0.8785625\n",
      "Train accuracy: 0.8843333333333333\n",
      "Final test accuracy: 0.5588333333333333\n"
     ]
    }
   ],
   "source": [
    "history_train = []\n",
    "history_test = []\n",
    "train_dataloader_len = len(train_dataloader)\n",
    "test_dataloader_len = len(test_dataloader)\n",
    "train_len = X_train.shape[0]\n",
    "test_len = X_test.shape[0]\n",
    "\n",
    "for epoch in range(15):  # loops over only 15 epochs because the model starts overfitting heavily after that\n",
    "    fnn_model1.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    train_accuracy = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        \n",
    "        inputs, label = data\n",
    "        inputs = inputs.float()\n",
    "        inputs, label = inputs.to(device), label.to(device)\n",
    "\n",
    "        model_optim.zero_grad()\n",
    "        output = fnn_model1(inputs)\n",
    "        output = output.to(device)\n",
    "        loss1 = CEloss(output, label)\n",
    "        loss1.backward()\n",
    "        model_optim.step()\n",
    "\n",
    "        train_loss += loss1.item()\n",
    "        _, output = torch.max(output, 1)\n",
    "        output = output.cpu().detach().numpy()\n",
    "        label = label.cpu().detach().numpy()\n",
    "        label = np.argmax(label, axis = 1)\n",
    "        train_accuracy += accuracy_score(label, output)\n",
    "    \n",
    "    train_loss = train_loss/train_dataloader_len\n",
    "    train_accuracy = train_accuracy/train_dataloader_len\n",
    "    history_train.append((train_loss, train_accuracy))\n",
    "    \n",
    "    fnn_model1.eval()   \n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    correct_test = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataloader, 0):\n",
    "            \n",
    "            inputs, label = data\n",
    "            inputs = inputs.float()\n",
    "            inputs, label = inputs.to(device), label.to(device)\n",
    "\n",
    "            pred = fnn_model1(inputs)\n",
    "            pred = pred.to(device)\n",
    "            loss2 = CEloss(pred, label)\n",
    "\n",
    "            test_loss += loss2.item()\n",
    "            _, pred = torch.max(pred, 1)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            label = label.cpu().detach().numpy()\n",
    "            label = np.argmax(label, axis = 1)\n",
    "            test_accuracy += accuracy_score(label, pred)\n",
    "            \n",
    "        test_loss = test_loss/test_dataloader_len\n",
    "        test_accuracy = test_accuracy/test_dataloader_len\n",
    "        history_test.append((test_loss, test_accuracy))\n",
    "    \n",
    "#     print(\"Epoch:\", (epoch+1))\n",
    "#     print(\"Train Loss:\", train_loss, \"\\tTrain Accuracy:\", train_accuracy)\n",
    "#     print(\"Test Loss:\", test_loss, \"\\tTest Accuracy:\", test_accuracy)\n",
    "#     print(\"=======================================================================================\")\n",
    "    print(\"Train accuracy:\", train_accuracy)\n",
    "\n",
    "print(\"Final test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151192df",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5230f01",
   "metadata": {},
   "source": [
    "#### PART A (Simple RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fd73aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limiting the maximum review length to 20 by truncating longer reviews and padding shorter reviews with a null value 0 (for creating our dataset)\n",
    "new_data = []\n",
    "sent = []\n",
    "\n",
    "for i in range(len(df['review_body'])):\n",
    "    split_sent = df['review_body'].values[i].split(' ')\n",
    "    for word in split_sent[:20]:\n",
    "        sent.append(word)\n",
    "\n",
    "    if len(split_sent) < 20:\n",
    "        for i in range(20-len(split_sent)):\n",
    "            sent.append('0')\n",
    "    \n",
    "    new_data.append(sent)\n",
    "    sent = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "952670a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_data = []\n",
    "sent = []\n",
    "\n",
    "for i in range(len(new_data)):\n",
    "    sent_part = new_data[i]\n",
    "    for word in sent_part:\n",
    "        try:\n",
    "            sent.append(word_2_vec[word])\n",
    "        except:\n",
    "            sent.append(np.zeros(300))\n",
    "    sent = np.array(sent)\n",
    "    rnn_data.append(sent)\n",
    "    sent = []\n",
    "rnn_data = np.array(rnn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f88f1c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 20, 300) (12000, 20, 300) (48000, 3) (12000, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(rnn_data, enc_data_label, test_size = 0.2, random_state=10)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "43526e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_dataset = Dataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fa3b52d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn = nn.RNN(300, 20, 1, batch_first = True, nonlinearity='relu')\n",
    "        self.fc1 = nn.Linear(20, 3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "         \n",
    "        x, hn = self.rnn(x)\n",
    "        x = nn.functional.relu(self.fc1(x[:, -1, :]))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c1d681a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNN().to(device)\n",
    "rnn_model = rnn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2c5e1601",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optim=torch.optim.Adam(rnn_model.parameters(), lr = 0.0001)\n",
    "CEloss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ec5acdab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.3328333333333333\n",
      "Train accuracy: 0.33447916666666666\n",
      "Train accuracy: 0.335375\n",
      "Train accuracy: 0.336\n",
      "Train accuracy: 0.338125\n",
      "Train accuracy: 0.34270833333333334\n",
      "Train accuracy: 0.35975\n",
      "Train accuracy: 0.37164583333333334\n",
      "Train accuracy: 0.3819375\n",
      "Train accuracy: 0.39460416666666664\n",
      "Train accuracy: 0.47091666666666665\n",
      "Train accuracy: 0.4885833333333333\n",
      "Train accuracy: 0.502875\n",
      "Train accuracy: 0.5130208333333334\n",
      "Train accuracy: 0.53025\n",
      "Train accuracy: 0.5389166666666667\n",
      "Train accuracy: 0.55425\n",
      "Train accuracy: 0.56275\n",
      "Train accuracy: 0.5678333333333333\n",
      "Train accuracy: 0.5717708333333333\n",
      "Train accuracy: 0.5772083333333333\n",
      "Train accuracy: 0.58175\n",
      "Train accuracy: 0.5845416666666666\n",
      "Train accuracy: 0.5869583333333334\n",
      "Train accuracy: 0.5897083333333333\n",
      "Train accuracy: 0.5927291666666666\n",
      "Train accuracy: 0.5946666666666667\n",
      "Train accuracy: 0.5997083333333333\n",
      "Train accuracy: 0.6\n",
      "Train accuracy: 0.6030833333333333\n",
      "Train accuracy: 0.6055833333333334\n",
      "Train accuracy: 0.6070416666666667\n",
      "Train accuracy: 0.609375\n",
      "Train accuracy: 0.6108541666666667\n",
      "Train accuracy: 0.6132291666666667\n",
      "Train accuracy: 0.6154166666666666\n",
      "Train accuracy: 0.6161875\n",
      "Train accuracy: 0.6167916666666666\n",
      "Train accuracy: 0.62025\n",
      "Train accuracy: 0.6223333333333333\n",
      "Final test accuracy: 0.6028333333333333\n"
     ]
    }
   ],
   "source": [
    "history_train = []\n",
    "history_test = []\n",
    "train_dataloader_len = len(train_dataloader)\n",
    "test_dataloader_len = len(test_dataloader)\n",
    "train_len = X_train.shape[0]\n",
    "test_len = X_test.shape[0]\n",
    "\n",
    "for epoch in range(40):  \n",
    "    rnn_model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    train_accuracy = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        \n",
    "        inputs, label = data\n",
    "        inputs = inputs.float()\n",
    "        inputs, label = inputs.to(device), label.to(device)\n",
    "\n",
    "        model_optim.zero_grad()\n",
    "        output = rnn_model(inputs)\n",
    "        output = output.to(device)\n",
    "        loss1 = CEloss(output, label)\n",
    "        loss1.backward()\n",
    "        model_optim.step()\n",
    "\n",
    "        train_loss += loss1.item()\n",
    "        _, output = torch.max(output, 1)\n",
    "        output = output.cpu().detach().numpy()\n",
    "        label = label.cpu().detach().numpy()\n",
    "        label = np.argmax(label, axis = 1)\n",
    "        train_accuracy += accuracy_score(label, output)\n",
    "    \n",
    "    train_loss = train_loss/train_dataloader_len\n",
    "    train_accuracy = train_accuracy/train_dataloader_len\n",
    "    history_train.append((train_loss, train_accuracy))\n",
    "    \n",
    "    rnn_model.eval()   \n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    correct_test = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataloader, 0):\n",
    "            \n",
    "            inputs, label = data\n",
    "            inputs = inputs.float()\n",
    "            inputs, label = inputs.to(device), label.to(device)\n",
    "\n",
    "            pred = rnn_model(inputs)\n",
    "            pred = pred.to(device)\n",
    "            loss2 = CEloss(pred, label)\n",
    "\n",
    "            test_loss += loss2.item()\n",
    "            _, pred = torch.max(pred, 1)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            label = label.cpu().detach().numpy()\n",
    "            label = np.argmax(label, axis = 1)\n",
    "            test_accuracy += accuracy_score(label, pred)\n",
    "            \n",
    "        test_loss = test_loss/test_dataloader_len\n",
    "        test_accuracy = test_accuracy/test_dataloader_len\n",
    "        history_test.append((test_loss, test_accuracy))\n",
    "    \n",
    "#     print(\"Epoch:\", (epoch+1))\n",
    "#     print(\"Train Loss:\", train_loss, \"\\tTrain Accuracy:\", train_accuracy)\n",
    "#     print(\"Test Loss:\", test_loss, \"\\tTest Accuracy:\", test_accuracy)\n",
    "#     print(\"=======================================================================================\")\n",
    "    print(\"Train accuracy:\", train_accuracy)\n",
    "\n",
    "print(\"Final test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad2985",
   "metadata": {},
   "source": [
    "##### FNN gives better test accuracy than Simple RNN. But the dataset (input) is different for Simple RNN and FNN. If the dataset would have been same, RNN would have given better test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f56135",
   "metadata": {},
   "source": [
    "#### PART B (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "05060f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gru = nn.GRU(300, 20, 1, batch_first = True)\n",
    "        self.fc1 = nn.Linear(20, 3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "         \n",
    "        x, hn = self.gru(x)\n",
    "        x = nn.functional.relu(self.fc1(x[:, -1, :]))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "eac09ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model = GRU().to(device)\n",
    "gru_model = gru_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ebecd4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optim=torch.optim.Adam(gru_model.parameters(), lr = 0.0001)\n",
    "CEloss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2b341e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.36666666666666664\n",
      "Train accuracy: 0.39789583333333334\n",
      "Train accuracy: 0.5310416666666666\n",
      "Train accuracy: 0.5660416666666667\n",
      "Train accuracy: 0.5832083333333333\n",
      "Train accuracy: 0.5949791666666666\n",
      "Train accuracy: 0.6066458333333333\n",
      "Train accuracy: 0.6150625\n",
      "Train accuracy: 0.6231458333333333\n",
      "Train accuracy: 0.6290625\n",
      "Train accuracy: 0.6328541666666667\n",
      "Train accuracy: 0.637875\n",
      "Train accuracy: 0.6425625\n",
      "Train accuracy: 0.6444166666666666\n",
      "Train accuracy: 0.6465833333333333\n",
      "Train accuracy: 0.6493958333333333\n",
      "Train accuracy: 0.6505416666666667\n",
      "Train accuracy: 0.6530833333333333\n",
      "Train accuracy: 0.654625\n",
      "Train accuracy: 0.6564375\n",
      "Train accuracy: 0.658375\n",
      "Train accuracy: 0.658875\n",
      "Train accuracy: 0.6618125\n",
      "Train accuracy: 0.6625833333333333\n",
      "Train accuracy: 0.6641458333333333\n",
      "Train accuracy: 0.6662291666666667\n",
      "Train accuracy: 0.6659166666666667\n",
      "Train accuracy: 0.668375\n",
      "Train accuracy: 0.6686041666666667\n",
      "Train accuracy: 0.6692916666666666\n",
      "Train accuracy: 0.6718958333333334\n",
      "Train accuracy: 0.6714375\n",
      "Train accuracy: 0.6729166666666667\n",
      "Train accuracy: 0.6736458333333334\n",
      "Train accuracy: 0.6748333333333333\n",
      "Train accuracy: 0.67575\n",
      "Train accuracy: 0.6777916666666667\n",
      "Train accuracy: 0.6782708333333334\n",
      "Train accuracy: 0.67875\n",
      "Train accuracy: 0.6797916666666667\n",
      "Final test accuracy: 0.6551666666666667\n"
     ]
    }
   ],
   "source": [
    "history_train = []\n",
    "history_test = []\n",
    "train_dataloader_len = len(train_dataloader)\n",
    "test_dataloader_len = len(test_dataloader)\n",
    "train_len = X_train.shape[0]\n",
    "test_len = X_test.shape[0]\n",
    "\n",
    "for epoch in range(40):  \n",
    "    gru_model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    train_accuracy = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        \n",
    "        inputs, label = data\n",
    "        inputs = inputs.float()\n",
    "        inputs, label = inputs.to(device), label.to(device)\n",
    "\n",
    "        model_optim.zero_grad()\n",
    "        output = gru_model(inputs)\n",
    "        output = output.to(device)\n",
    "        loss1 = CEloss(output, label)\n",
    "        loss1.backward()\n",
    "        model_optim.step()\n",
    "\n",
    "        train_loss += loss1.item()\n",
    "        _, output = torch.max(output, 1)\n",
    "        output = output.cpu().detach().numpy()\n",
    "        label = label.cpu().detach().numpy()\n",
    "        label = np.argmax(label, axis = 1)\n",
    "        train_accuracy += accuracy_score(label, output)\n",
    "    \n",
    "    train_loss = train_loss/train_dataloader_len\n",
    "    train_accuracy = train_accuracy/train_dataloader_len\n",
    "    history_train.append((train_loss, train_accuracy))\n",
    "    \n",
    "    gru_model.eval()   \n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    correct_test = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataloader, 0):\n",
    "            \n",
    "            inputs, label = data\n",
    "            inputs = inputs.float()\n",
    "            inputs, label = inputs.to(device), label.to(device)\n",
    "\n",
    "            pred = gru_model(inputs)\n",
    "            pred = pred.to(device)\n",
    "            loss2 = CEloss(pred, label)\n",
    "\n",
    "            test_loss += loss2.item()\n",
    "            _, pred = torch.max(pred, 1)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            label = label.cpu().detach().numpy()\n",
    "            label = np.argmax(label, axis = 1)\n",
    "            test_accuracy += accuracy_score(label, pred)\n",
    "            \n",
    "        test_loss = test_loss/test_dataloader_len\n",
    "        test_accuracy = test_accuracy/test_dataloader_len\n",
    "        history_test.append((test_loss, test_accuracy))\n",
    "    \n",
    "#     print(\"Epoch:\", (epoch+1))\n",
    "#     print(\"Train Loss:\", train_loss, \"\\tTrain Accuracy:\", train_accuracy)\n",
    "#     print(\"Test Loss:\", test_loss, \"\\tTest Accuracy:\", test_accuracy)\n",
    "#     print(\"=======================================================================================\")\n",
    "    print(\"Train accuracy:\", train_accuracy)\n",
    "print(\"Final test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069286c5",
   "metadata": {},
   "source": [
    "#### PART C (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7479b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(300, 20, 1, batch_first = True)\n",
    "        self.fc1 = nn.Linear(20, 3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "         \n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        x = nn.functional.relu(self.fc1(x[:, -1, :]))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ccac73ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM().to(device)\n",
    "lstm_model = lstm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a589d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optim=torch.optim.Adam(lstm_model.parameters(), lr = 0.0003)\n",
    "CEloss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7bd1b966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.4464375\n",
      "Train accuracy: 0.5387916666666667\n",
      "Train accuracy: 0.5848125\n",
      "Train accuracy: 0.6065208333333333\n",
      "Train accuracy: 0.621125\n",
      "Train accuracy: 0.6314375\n",
      "Train accuracy: 0.6374166666666666\n",
      "Train accuracy: 0.6424791666666667\n",
      "Train accuracy: 0.6488541666666666\n",
      "Train accuracy: 0.65475\n",
      "Train accuracy: 0.6585416666666667\n",
      "Train accuracy: 0.6615416666666667\n",
      "Train accuracy: 0.6640208333333333\n",
      "Train accuracy: 0.6696666666666666\n",
      "Train accuracy: 0.6710833333333334\n",
      "Train accuracy: 0.6739375\n",
      "Train accuracy: 0.6747083333333334\n",
      "Train accuracy: 0.6781041666666666\n",
      "Train accuracy: 0.6815625\n",
      "Train accuracy: 0.6819166666666666\n",
      "Train accuracy: 0.6850625\n",
      "Train accuracy: 0.689\n",
      "Train accuracy: 0.6889791666666667\n",
      "Train accuracy: 0.6913958333333333\n",
      "Train accuracy: 0.6914791666666666\n",
      "Train accuracy: 0.695375\n",
      "Train accuracy: 0.6964791666666666\n",
      "Train accuracy: 0.697625\n",
      "Train accuracy: 0.698875\n",
      "Train accuracy: 0.7006875\n",
      "Final test accuracy: 0.6625833333333333\n"
     ]
    }
   ],
   "source": [
    "history_train = []\n",
    "history_test = []\n",
    "train_dataloader_len = len(train_dataloader)\n",
    "test_dataloader_len = len(test_dataloader)\n",
    "train_len = X_train.shape[0]\n",
    "test_len = X_test.shape[0]\n",
    "\n",
    "for epoch in range(30): \n",
    "    lstm_model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    train_accuracy = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        \n",
    "        inputs, label = data\n",
    "        inputs = inputs.float()\n",
    "        inputs, label = inputs.to(device), label.to(device)\n",
    "\n",
    "        model_optim.zero_grad()\n",
    "        output = lstm_model(inputs)\n",
    "        output = output.to(device)\n",
    "        loss1 = CEloss(output, label)\n",
    "        loss1.backward()\n",
    "        model_optim.step()\n",
    "\n",
    "        train_loss += loss1.item()\n",
    "        _, output = torch.max(output, 1)\n",
    "        output = output.cpu().detach().numpy()\n",
    "        label = label.cpu().detach().numpy()\n",
    "        label = np.argmax(label, axis = 1)\n",
    "        train_accuracy += accuracy_score(label, output)\n",
    "    \n",
    "    train_loss = train_loss/train_dataloader_len\n",
    "    train_accuracy = train_accuracy/train_dataloader_len\n",
    "    history_train.append((train_loss, train_accuracy))\n",
    "    \n",
    "    lstm_model.eval()   \n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    correct_test = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataloader, 0):\n",
    "            \n",
    "            inputs, label = data\n",
    "            inputs = inputs.float()\n",
    "            inputs, label = inputs.to(device), label.to(device)\n",
    "\n",
    "            pred = lstm_model(inputs)\n",
    "            pred = pred.to(device)\n",
    "            loss2 = CEloss(pred, label)\n",
    "\n",
    "            test_loss += loss2.item()\n",
    "            _, pred = torch.max(pred, 1)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            label = label.cpu().detach().numpy()\n",
    "            label = np.argmax(label, axis = 1)\n",
    "            test_accuracy += accuracy_score(label, pred)\n",
    "            \n",
    "        test_loss = test_loss/test_dataloader_len\n",
    "        test_accuracy = test_accuracy/test_dataloader_len\n",
    "        history_test.append((test_loss, test_accuracy))\n",
    "    \n",
    "#     print(\"Epoch:\", (epoch+1))\n",
    "#     print(\"Train Loss:\", train_loss, \"\\tTrain Accuracy:\", train_accuracy)\n",
    "#     print(\"Test Loss:\", test_loss, \"\\tTest Accuracy:\", test_accuracy)\n",
    "#     print(\"=======================================================================================\")\n",
    "    print(\"Train accuracy:\", train_accuracy)\n",
    "\n",
    "print(\"Final test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d98bf",
   "metadata": {},
   "source": [
    "##### We can conclude that GRU and LSTM give better results than Simple RNN for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d3f0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f08e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
